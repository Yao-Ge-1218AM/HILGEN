{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "step 1: Extract all concepts from training data"
      ],
      "metadata": {
        "id": "ZKCmCi9REY9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  i = 0\n",
        "  tmp = []\n",
        "  entity = set()\n",
        "  for i in range(len(lines)-1):\n",
        "    line = lines[i].strip(\"\\n\")\n",
        "    next_line = lines[i+1].strip(\"\\n\")\n",
        "    if(line.endswith(\"B-Disease\")):\n",
        "      tmp = []\n",
        "      if(next_line.endswith(\"I-Disease\")):     # need to change I-GENE\n",
        "        tmp.append(line.split(\" \")[0])\n",
        "        while(next_line.endswith(\"I-Disease\")):\n",
        "          tmp.append(next_line.split(\" \")[0])\n",
        "          i+=1\n",
        "          next_line = lines[i+1].strip(\"\\n\")\n",
        "        phrase = \"\"\n",
        "        for j in range(len(tmp)-1):\n",
        "          #wf.write(tmp[j] + \" \")\n",
        "          phrase += tmp[j]\n",
        "          phrase += \" \"\n",
        "        #wf.write(tmp[j+1] + \"\\n\")\n",
        "        phrase += tmp[j+1]\n",
        "        #phrase += \"\\n\"\n",
        "        entity.add(phrase)\n",
        "      else:\n",
        "        entity.add(line.split(\" \")[0])\n",
        "        #wf.wirte(line + \"\\n\")\n",
        "print(entity)\n",
        "with open(\"/content/train-disease-entities.txt\",\"w\") as wf:\n",
        "  for ent in entity:\n",
        "    print(ent)\n",
        "    wf.write(ent + \"\\n\")"
      ],
      "metadata": {
        "id": "DiC-dEKm8S6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: write the scripts for search for all related concepts\n",
        "\n"
      ],
      "metadata": {
        "id": "pmTnRMuFMf71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/related-concepts/"
      ],
      "metadata": {
        "id": "gGQpE0chqzj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/related-concepts/"
      ],
      "metadata": {
        "id": "3gx-RQyWGpj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Each entity needs to create a file, and it's file name is the same as entity name.\n",
        "\n",
        "2. Use the UMLS API, to creat the url for searching.\n",
        "\n",
        "3. According to each url, get a json file.\n",
        "\n",
        "4. Parse json file, get related concepts files corresponding to each entity, but only top 10 are enough.\n",
        "\n",
        "5. Cui is saved as a dictionary for later use to look up parents and children.\n"
      ],
      "metadata": {
        "id": "5tiB3LuAH3AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "apikey = \"7e7e253a-1168-4530-b8b4-24cb7daaa701\"\n",
        "api_url = \"https://uts-ws.nlm.nih.gov/rest/search/current\"\n",
        "\n",
        "related_concepts_path = \"/content/related-concepts/\"\n",
        "\n",
        "with open(\"/content/train-disease-entities.txt\",\"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      #print(line)\n",
        "      string = line.strip(\"\\n\")\n",
        "      url = api_url + \"?apiKey=\" + apikey + \"&string=\" + string\n",
        "      response = requests.get(url)\n",
        "\n",
        "      data = json.loads(response.content)\n",
        "\n",
        "      with open(related_concepts_path + line.strip(\"\\n\") + \".txt\",\"a\") as wf:\n",
        "        i = 1\n",
        "        for concept in data['result']['results']:\n",
        "          if(i<=10):\n",
        "            #print(concept['name'])\n",
        "            wf.write(concept['name']+\"\\n\")\n",
        "          i+=1\n"
      ],
      "metadata": {
        "id": "nZ8TKc1w_j1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: write the scripts for search for parents and children\n"
      ],
      "metadata": {
        "id": "vjNM6EOFIHEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/parents/"
      ],
      "metadata": {
        "id": "rYJ7J-koPd_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/parents/"
      ],
      "metadata": {
        "id": "h60O06e6PfqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get the cuis of the given concepts\n",
        "\n"
      ],
      "metadata": {
        "id": "RWu63ZRShIPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "apikey = \"7e7e253a-1168-4530-b8b4-24cb7daaa701\"\n",
        "api_url = \"https://uts-ws.nlm.nih.gov/rest/search/current\"\n",
        "\n",
        "related_concepts_path = \"/content/related-concepts/\"\n",
        "\n",
        "with open(\"/content/train-disease-entities.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-disease-uis.txt\",\"w\") as wf:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      #print(line)\n",
        "      string = line.strip(\"\\n\")\n",
        "      url = api_url + \"?apiKey=\" + apikey + \"&string=\" + string + \"&returnIdType=code&sabs=SNOMEDCT_US\"\n",
        "      response = requests.get(url)\n",
        "\n",
        "      #print(string)\n",
        "      data = json.loads(response.content)\n",
        "      if(len(data['result']['results'])!=0):\n",
        "        wf.write(string + \"\\t\" + data['result']['results'][0]['ui']+\"\\n\")\n",
        "        #print(data['result']['results'])"
      ],
      "metadata": {
        "id": "3IDAqlTITDQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find its parents and children according to cuis. Only top 10 are enough."
      ],
      "metadata": {
        "id": "wXvz1JlghK_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "apikey = \"7e7e253a-1168-4530-b8b4-24cb7daaa701\"\n",
        "api_url = \"https://uts-ws.nlm.nih.gov/rest/content/current/source/SNOMEDCT_US/\"\n",
        "\n",
        "parents_children_path = \"/content/parents/\"\n",
        "\n",
        "with open(\"/content/train-disease-uis.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    #print(line)\n",
        "    string = line.strip(\"\\n\").split(\"\\t\")[1]\n",
        "    #print(string)\n",
        "    url = api_url + string + \"/parents?apiKey=\" + apikey\n",
        "    #print(url)\n",
        "    response = requests.get(url)\n",
        "\n",
        "    data = json.loads(response.content)\n",
        "\n",
        "    with open(parents_children_path + line.strip(\"\\n\").split(\"\\t\")[0] + \".txt\",\"a\") as wf:\n",
        "      i = 1\n",
        "      for concept in data['result']:\n",
        "        if(i<=10):\n",
        "          #print(concept['name'])\n",
        "          wf.write(concept['name']+\"\\n\")\n",
        "        i+=1\n",
        "\n",
        "    url = api_url + string + \"/children?apiKey=\" + apikey\n",
        "    response = requests.get(url)\n",
        "\n",
        "    data = json.loads(response.content)\n",
        "    #print(url,data)\n",
        "\n",
        "    file_name = parents_children_path + line.strip(\"\\n\").split(\"\\t\")[0] + \".txt\"\n",
        "    if(os.path.exists(file_name)):\n",
        "      with open(file_name,\"r\") as rf:\n",
        "        lines = rf.readlines()\n",
        "        i = len(lines)+1\n",
        "        #print(file_name,i)\n",
        "    else:\n",
        "      i = 1\n",
        "    with open(file_name,\"a\") as wf:\n",
        "      #if(exists, i = lens; else: i = 0)\n",
        "      if('result' in data):\n",
        "        for concept in data['result']:\n",
        "          if(i<=10):\n",
        "            #print(concept['name'])\n",
        "            wf.write(concept['name']+\"\\n\")\n",
        "          i+=1\n"
      ],
      "metadata": {
        "id": "SezXkqAFPMrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: write the scripts for search for siblings\n"
      ],
      "metadata": {
        "id": "QM2C9UzbjbHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/siblings/"
      ],
      "metadata": {
        "id": "DhU47nfOriHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/siblings/"
      ],
      "metadata": {
        "id": "JUIN9p_Hrjyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get the cuis of the parent of the given concept\n",
        "\n"
      ],
      "metadata": {
        "id": "ujdKOJh-rK1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "apikey = \"7e7e253a-1168-4530-b8b4-24cb7daaa701\"\n",
        "api_url = \"https://uts-ws.nlm.nih.gov/rest/content/current/source/SNOMEDCT_US/\"\n",
        "\n",
        "with open(\"/content/train-disease-uis.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-disease-parents-uis.txt\",\"w\") as wf:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      #print(line)\n",
        "      string = line.strip(\"\\n\").split(\"\\t\")[1]\n",
        "      #print(string)\n",
        "      url = api_url + string + \"/parents?apiKey=\" + apikey\n",
        "      #print(url)\n",
        "      response = requests.get(url)\n",
        "\n",
        "      data = json.loads(response.content)\n",
        "\n",
        "      if('result' in data):\n",
        "        wf.write(line.strip(\"\\n\").split(\"\\t\")[0] + \"\\t\" + data['result'][0]['ui']+\"\\n\")"
      ],
      "metadata": {
        "id": "uyxffMBhjkDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find its children according to cuis. Only top 10 are enough."
      ],
      "metadata": {
        "id": "BhZwf6yyrMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "apikey = \"7e7e253a-1168-4530-b8b4-24cb7daaa701\"\n",
        "api_url = \"https://uts-ws.nlm.nih.gov/rest/content/current/source/SNOMEDCT_US/\"\n",
        "\n",
        "parents_children_path = \"/content/siblings/\"\n",
        "\n",
        "with open(\"/content/train-disease-parents-uis.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    #print(line)\n",
        "    string = line.strip(\"\\n\").split(\"\\t\")[1]\n",
        "    #print(string)\n",
        "    url = api_url + string + \"/children?apiKey=\" + apikey\n",
        "    response = requests.get(url)\n",
        "\n",
        "    data = json.loads(response.content)\n",
        "    #print(url,data)\n",
        "\n",
        "    file_name = parents_children_path + line.strip(\"\\n\").split(\"\\t\")[0] + \".txt\"\n",
        "    i = 1\n",
        "    with open(file_name,\"a\") as wf:\n",
        "      if('result' in data):\n",
        "        for concept in data['result']:\n",
        "          if(i<=10):\n",
        "            #print(concept['name'])\n",
        "            wf.write(concept['name']+\"\\n\")\n",
        "          i+=1\n"
      ],
      "metadata": {
        "id": "p1OJzKmKqxZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Replace given concepts with UMLS concepts -- process training data"
      ],
      "metadata": {
        "id": "otcLUPzUrvPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Extract sentences and BIOs seperately from training data"
      ],
      "metadata": {
        "id": "rxB-2wGyvvI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train-5-shot.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-sents.txt\",\"a\") as wf:\n",
        "    with open(\"/content/train-BIOs.txt\",\"a\") as wf1:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        if(line!=\"\\n\"):\n",
        "          new_line = line.strip(\"\\n\").split(\" \")\n",
        "          wf.write(new_line[0] + \" \")\n",
        "          wf1.write(new_line[1] + \" \")\n",
        "        else:\n",
        "          wf.write(\"\\n\")\n",
        "          wf1.write(\"\\n\")"
      ],
      "metadata": {
        "id": "TK4qAJHfvgvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Replace given concepts with UMLS concepts"
      ],
      "metadata": {
        "id": "mNsV_ynnvhVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "index = []\n",
        "\n",
        "data_dir = \"/content/parents/\"\n",
        "#data_dir = \"/content/siblings/\"\n",
        "#data_dir = \"/content/related_concepts/\"\n",
        "for file in os.listdir(data_dir):\n",
        "  with open(\"/content/train-sents.txt\",\"r\") as f:\n",
        "    with open(data_dir + file,\"r\") as f1:\n",
        "      concept = file[:-4]\n",
        "      #print(concept)\n",
        "      with open(\"/content/train-5-shot.txt\",\"a\") as wf:\n",
        "        lines = f.readlines()\n",
        "        lines1 = f1.readlines()\n",
        "        #print(len(lines1))\n",
        "        count = -1\n",
        "        for line in lines:\n",
        "          count+=1\n",
        "          if(concept in line):\n",
        "            #print(count, line)\n",
        "            new_line = line.strip(\"\\n\").split()\n",
        "            start = int(line.find(concept))\n",
        "            c = 0\n",
        "            for i in range(start):\n",
        "              if(line[i]==\" \"):\n",
        "                c+=1                       # c: 第几个单词开始为concept\n",
        "            lens = len(concept.split())\n",
        "            #print(c,lens)                  # 从第c个单词开始，到 c+lens-1 的单词都需要我自己给BIO，其他的使用原本储存的BIO\n",
        "            with open(\"/content/train-BIOs.txt\",\"r\") as f2:\n",
        "              lines2 = f2.readlines()\n",
        "              originalBIO = lines2[count]\n",
        "              BIO_list = originalBIO.split(\" \")\n",
        "            if(len(lines1)>10):\n",
        "              index = random.sample(range(0,len(lines1)),10)\n",
        "            for l in range(len(lines1)):\n",
        "              if(len(index)==0 or (len(index)!=0 and (l in index))):\n",
        "                line1 = lines1[l]\n",
        "                #print(line1)\n",
        "                new_line1 = line1.strip(\"\\n\").split(\" \")\n",
        "                lens2 = len(new_line1)\n",
        "                #print(lens2, new_line + \"\\n\")\n",
        "                for j in range(c):\n",
        "                  wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "                for k in range(lens2):\n",
        "                  if(k==0):\n",
        "                    wf.write(new_line1[k] + \" \" + \"B-Disease\" + \"\\n\")\n",
        "                  else:\n",
        "                    wf.write(new_line1[k] + \" \" + \"I-Disease\" + \"\\n\")\n",
        "                for j in range(c+lens, len(BIO_list)-1):\n",
        "                  wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "                wf.write(\"\\n\")\n",
        "            break"
      ],
      "metadata": {
        "id": "rOJ0r2Ztvhmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}