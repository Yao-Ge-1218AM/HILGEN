{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "当我被逼无奈开始思考到底怎么通过写代码的方式换掉那些可恶的词！！！\n",
        "\n",
        "我觉得可以：\n",
        "\n",
        "1. 先找到那一段\n",
        "\n",
        "2. 然后还原成文字纯享版\n",
        "\n",
        "3. 然后替换关键词\n",
        "\n",
        "4. 最后再整合成BIO的格式\n",
        "\n",
        "5. 加入到文件的最后，齐活了"
      ],
      "metadata": {
        "id": "iic1B1p9qBfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "提取given concept"
      ],
      "metadata": {
        "id": "bkSX1P__rwo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FvB6kWp5KD",
        "outputId": "e5a3cb19-3423-4588-f501-9b0cc4bdf146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Angelman syndrome\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/Angelman.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    concept = line.strip(\"\\n\")\n",
        "    break\n",
        "print(concept)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BIO to sentences"
      ],
      "metadata": {
        "id": "AMhWfHyaBQNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train-5-shot.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-sents.txt\",\"a\") as wf:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      if(line!=\"\\n\"):\n",
        "        new_line = line.strip(\"\\n\").split(\" \")\n",
        "        wf.write(new_line[0] + \" \")\n",
        "      else:\n",
        "        wf.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "r5OyV4O3r7Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train-5-shot.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-BIOs.txt\",\"a\") as wf:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      if(line!=\"\\n\"):\n",
        "        new_line = line.strip(\"\\n\").split(\" \")\n",
        "        wf.write(new_line[1] + \" \")\n",
        "      else:\n",
        "        wf.write(\"\\n\")"
      ],
      "metadata": {
        "id": "OOWRdJbhFaho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "找到given cencept 在training data中的位置，再整合成BIO的格式"
      ],
      "metadata": {
        "id": "wNvGIxfMr0wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train-sents.txt\",\"r\") as f:\n",
        "  with open(\"/content/related-concepts\",\"r\") as f1:\n",
        "    with open(\"/content/train-add.txt\",\"w\") as wf:\n",
        "      lines = f.readlines()\n",
        "      lines1 = f1.readlines()\n",
        "      count = -1\n",
        "      for line in lines:\n",
        "        count+=1\n",
        "        if(concept in line):\n",
        "          #print(count, line)\n",
        "          new_line = line.strip(\"\\n\").split()\n",
        "          start = int(line.find(concept))\n",
        "          c = 0\n",
        "          for i in range(start):\n",
        "            if(line[i]==\" \"):\n",
        "              c+=1                       # c: 第几个单词开始为concept\n",
        "          lens = len(concept.split())\n",
        "          #print(c,lens)                  # 从第c个单词开始，到 c+lens-1 的单词都需要我自己给BIO，其他的使用原本储存的BIO\n",
        "          with open(\"/content/train-BIOs.txt\",\"r\") as f2:\n",
        "            lines2 = f2.readlines()\n",
        "            originalBIO = lines2[count]\n",
        "            BIO_list = originalBIO.split(\" \")\n",
        "          for line1 in lines1:\n",
        "            new_line1 = line1.strip(\"\\n\").split(\" \")\n",
        "            lens2 = len(new_line1)\n",
        "            #print(lens2, new_line + \"\\n\")\n",
        "            for j in range(c):\n",
        "              wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "            for k in range(lens2):\n",
        "              if(k==0):\n",
        "                wf.write(new_line1[k] + \" \" + \"B-SpecificDisease\" + \"\\n\")\n",
        "              else:\n",
        "                wf.write(new_line1[k] + \" \" + \"I-SpecificDisease\" + \"\\n\")\n",
        "            for j in range(c+lens, len(BIO_list)-1):\n",
        "              wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "            wf.write(\"\\n\")"
      ],
      "metadata": {
        "id": "yHzhz8AgCjos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "整合，做所有文件的"
      ],
      "metadata": {
        "id": "4q_pP8EtSnLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train-5-shot.txt\",\"r\") as f:\n",
        "  with open(\"/content/train-sents.txt\",\"a\") as wf:\n",
        "    with open(\"/content/train-BIOs.txt\",\"a\") as wf1:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        if(line!=\"\\n\"):\n",
        "          new_line = line.strip(\"\\n\").split(\" \")\n",
        "          wf.write(new_line[0] + \" \")\n",
        "          wf1.write(new_line[1] + \" \")\n",
        "        else:\n",
        "          wf.write(\"\\n\")\n",
        "          wf1.write(\"\\n\")"
      ],
      "metadata": {
        "id": "0hTL13VbSpES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/parents.zip"
      ],
      "metadata": {
        "id": "RmiC3qNLTKwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/related_concepts.zip"
      ],
      "metadata": {
        "id": "tlNhGGSSyJse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ced73e2-6239-4473-e09f-d8420ab36716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/related_concepts.zip\n",
            "  inflating: related-concepts/Angelman syndrome.txt  \n",
            " extracting: related-concepts/brittle nails with prominent longitudinal grooves.txt  \n",
            "  inflating: related-concepts/colorectal carcinoma).txt  \n",
            "  inflating: related-concepts/coronal craniosynostosis..txt  \n",
            "  inflating: related-concepts/DMPK haploinsufficiency).txt  \n",
            "  inflating: related-concepts/frontonasal dysplasia.txt  \n",
            "  inflating: related-concepts/hemangioblastomas.txt  \n",
            "  inflating: related-concepts/leptin.txt  \n",
            "  inflating: related-concepts/obesity..txt  \n",
            "  inflating: related-concepts/ovarian cancers..txt  \n",
            "  inflating: related-concepts/pancreatic malignancies,.txt  \n",
            "  inflating: related-concepts/phaeochromocytomas,.txt  \n",
            " extracting: related-concepts/Prader-Willi syndrome.txt  \n",
            "  inflating: related-concepts/renal.txt  \n",
            "  inflating: related-concepts/sporadic ovarian tumours..txt  \n",
            "  inflating: related-concepts/Von Hippel-Lindau disease.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/related_concepts"
      ],
      "metadata": {
        "id": "BhkFGR7MC5xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/siblings.zip"
      ],
      "metadata": {
        "id": "-GfeIksd3H6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b012564b-d79f-46d8-fdba-f790db0814d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/siblings.zip\n",
            "  inflating: siblings/Angelman syndrome.txt  \n",
            "  inflating: siblings/brittle nails with prominent longitudinal grooves.txt  \n",
            "  inflating: siblings/colorectal carcinoma).txt  \n",
            "  inflating: siblings/coronal craniosynostosis..txt  \n",
            "  inflating: siblings/DMPK haploinsufficiency).txt  \n",
            "  inflating: siblings/frontonasal dysplasia.txt  \n",
            "  inflating: siblings/hemangioblastomas.txt  \n",
            "  inflating: siblings/leptin.txt     \n",
            "  inflating: siblings/obesity..txt   \n",
            "  inflating: siblings/ovarian cancers..txt  \n",
            "  inflating: siblings/pancreatic malignancies,.txt  \n",
            "  inflating: siblings/phaeochromocytomas,.txt  \n",
            "  inflating: siblings/Prader-Willi syndrome.txt  \n",
            "  inflating: siblings/renal.txt      \n",
            "  inflating: siblings/sporadic ovarian tumours..txt  \n",
            "  inflating: siblings/Von Hippel-Lindau disease.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf bc5cdr-parents.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMerqYRntGnf",
        "outputId": "77061107-ade9-49b4-a4b2-a4ecb370928f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content/parents/\n",
            "content/parents/hypertensive.txt\n",
            "content/parents/toxicity.txt\n",
            "content/parents/hypotensive.txt\n",
            "content/parents/granulocytopenia.txt\n",
            "content/parents/hyperalgesia.txt\n",
            "content/parents/atrial tachycardia.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/parents"
      ],
      "metadata": {
        "id": "fxsPCtnLz9AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/siblings"
      ],
      "metadata": {
        "id": "v92x6fMpzyyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/clinical.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5-iRVwWgcHW",
        "outputId": "38b257df-579d-470e-af3f-28af2d5bd710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/clinical.zip\n",
            "  inflating: clinical/addiction.txt  \n",
            "  inflating: clinical/codeine addict.txt  \n",
            "  inflating: clinical/I just called a 1800 number and was in a rehab the next day.txt  \n",
            "  inflating: clinical/Makes the pain tolerable.txt  \n",
            "  inflating: clinical/tweaking.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/social.zip"
      ],
      "metadata": {
        "id": "WfD90Ax-g8h0",
        "outputId": "f55e16e0-df92-42da-d482-38b7280fb110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/social.zip\n",
            "  inflating: social/A long time ago I once had blacked out for basically a couple  few days.txt  \n",
            "  inflating: social/but the shame, the isolation and the heavy secrets were..txt  \n",
            "  inflating: social/I was worried I'd lose my job because I'd be unable to function for a week.txt  \n",
            "  inflating: social/she hadn't been home for days.txt  \n",
            "  inflating: social/talkative.txt    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "index = []\n",
        "\n",
        "#data_dir = \"/content/parents/\"\n",
        "data_dir = \"/content/siblings/\"\n",
        "#data_dir = \"/content/related_concepts/\"\n",
        "#data_dir = \"/content/clinical/\"\n",
        "#data_dir = \"/content/social/\"\n",
        "for file in os.listdir(data_dir):\n",
        "  with open(\"/content/train-sents.txt\",\"r\") as f:\n",
        "    with open(data_dir + file,\"r\") as f1:\n",
        "      concept = file[:-4]\n",
        "      #print(concept)\n",
        "      with open(\"/content/train-5-shot.txt\",\"a\") as wf:\n",
        "        lines = f.readlines()\n",
        "        lines1 = f1.readlines()\n",
        "        #print(len(lines1))\n",
        "        count = -1\n",
        "        for line in lines:\n",
        "          count+=1\n",
        "          if(concept in line):\n",
        "            #print(count, line)\n",
        "            new_line = line.strip(\"\\n\").split()\n",
        "            start = int(line.find(concept))\n",
        "            c = 0\n",
        "            for i in range(start):\n",
        "              if(line[i]==\" \"):\n",
        "                c+=1                       # c: 第几个单词开始为concept\n",
        "            lens = len(concept.split())\n",
        "            #print(c,lens)                  # 从第c个单词开始，到 c+lens-1 的单词都需要我自己给BIO，其他的使用原本储存的BIO\n",
        "            with open(\"/content/train-BIOs.txt\",\"r\") as f2:\n",
        "              lines2 = f2.readlines()\n",
        "              originalBIO = lines2[count]\n",
        "              BIO_list = originalBIO.split(\" \")\n",
        "            if(len(lines1)>10):\n",
        "              index = random.sample(range(0,len(lines1)),10)\n",
        "            for l in range(len(lines1)):\n",
        "              if(len(index)==0 or (len(index)!=0 and (l in index))):\n",
        "                line1 = lines1[l]\n",
        "                #print(line1)\n",
        "                new_line1 = line1.strip(\"\\n\").split(\" \")\n",
        "                lens2 = len(new_line1)\n",
        "                #print(lens2, new_line + \"\\n\")\n",
        "                for j in range(c):\n",
        "                  wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "                for k in range(lens2):\n",
        "                  if(k==0):\n",
        "                    wf.write(new_line1[k] + \" \" + \"B-SpecificDisease\" + \"\\n\")\n",
        "                  else:\n",
        "                    wf.write(new_line1[k] + \" \" + \"I-SpecificDisease\" + \"\\n\")\n",
        "                for j in range(c+lens, len(BIO_list)-1):\n",
        "                  wf.write(new_line[j] + \" \" + BIO_list[j] + \"\\n\")\n",
        "                wf.write(\"\\n\")\n",
        "            break"
      ],
      "metadata": {
        "id": "pD9U59HsWvwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}